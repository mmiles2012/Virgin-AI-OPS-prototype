import requests
import pandas as pd
import json
import time
from bs4 import BeautifulSoup
import csv
from urllib.parse import urljoin, urlparse
import logging
from datetime import datetime
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import List, Dict, Optional
import os

# Setup logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('airport_scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class FacilityInfo:
    """Data structure for detailed airport facility information"""
    icao: str
    airport_name: str
    ground_handlers: List[str]
    maintenance_providers: List[str]
    fuel_suppliers: List[str]
    cargo_handlers: List[str]
    catering_services: List[str]
    fbo_services: List[str]
    customs_hours: str
    immigration_services: str
    contact_info: Dict[str, str]
    operating_hours: str
    source_url: str
    last_updated: str

class CombinedAirportScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # Create output directory
        self.output_dir = 'airport_data_output'
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Service provider keywords for detailed scraping
        self.ground_handlers = [
            'dnata', 'swissport', 'menzies', 'worldwide flight services', 'wfs',
            'celebi', 'aviapartner', 'groundforce', 'bags', 'servisair',
            'globe ground', 'airport handling', 'signature flight support'
        ]
        
        self.maintenance_providers = [
            'lufthansa technik', 'st aerospace', 'aar corp', 'ameco',
            'tap maintenance', 'turkish technic', 'mro', 'sr technics',
            'haeco', 'jet maintenance', 'aircraft maintenance'
        ]
        
        self.fuel_suppliers = [
            'shell aviation', 'bp aviation', 'total aviation', 'chevron',
            'world energy', 'avgas', 'jet fuel', 'fuel services',
            'aviation fuel', 'into-plane'
        ]
    
    # ============ PHASE 1: FOUNDATION DATA COLLECTION ============
    
    def download_ourairports_data(self):
        """Download CSV data from OurAirports.com"""
        logger.info("Phase 1: Downloading OurAirports data...")
        
        urls = {
            'airports': 'http://ourairports.com/data/airports.csv',
            'countries': 'http://ourairports.com/data/countries.csv',
            'regions': 'http://ourairports.com/data/regions.csv',
            'runways': 'http://ourairports.com/data/runways.csv',
            'navaids': 'http://ourairports.com/data/navaids.csv'
        }
        
        data = {}
        for name, url in urls.items():
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                # Save raw CSV
                filepath = os.path.join(self.output_dir, f'ourairports_{name}.csv')
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                
                # Parse into DataFrame
                data[name] = pd.read_csv(url)
                logger.info(f"Downloaded {name}: {len(data[name])} records")
                
            except Exception as e:
                logger.error(f"Error downloading {name}: {e}")
        
        return data
    
    def download_openflights_data(self):
        """Download data from OpenFlights"""
        logger.info("Phase 1: Downloading OpenFlights data...")
        
        urls = {
            'airports': 'https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat',
            'airlines': 'https://raw.githubusercontent.com/jpatokal/openflights/master/data/airlines.dat',
            'routes': 'https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat',
            'countries': 'https://raw.githubusercontent.com/jpatokal/openflights/master/data/countries.dat'
        }
        
        airport_columns = ['id', 'name', 'city', 'country', 'iata', 'icao', 'latitude', 'longitude', 
                          'altitude', 'timezone', 'dst', 'tz_database', 'type', 'source']
        
        data = {}
        for name, url in urls.items():
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                # Save raw data
                filepath = os.path.join(self.output_dir, f'openflights_{name}.dat')
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                
                # Parse data
                if name == 'airports':
                    data[name] = pd.read_csv(url, names=airport_columns, na_values=['\\N'])
                else:
                    data[name] = pd.read_csv(url, na_values=['\\N'])
                
                logger.info(f"Downloaded {name}: {len(data[name])} records")
                
            except Exception as e:
                logger.error(f"Error downloading {name}: {e}")
        
        return data
    
    def scrape_openaip_data(self):
        """Scrape OpenAIP data"""
        logger.info("Phase 1: Scraping OpenAIP data...")
        
        try:
            base_url = "https://www.openaip.net/api/countries"
            response = self.session.get(base_url, timeout=30)
            response.raise_for_status()
            countries = response.json()
            
            airport_data = []
            
            for country in countries[:15]:  # Limit to avoid overwhelming the API
                country_code = country.get('code', '')
                logger.info(f"Fetching OpenAIP data for {country.get('name', country_code)}")
                
                airports_url = f"https://www.openaip.net/api/airports?country={country_code}"
                
                try:
                    airport_response = self.session.get(airports_url, timeout=30)
                    airport_response.raise_for_status()
                    airports = airport_response.json()
                    
                    for airport in airports:
                        airport_data.append({
                            'country': country.get('name', ''),
                            'country_code': country_code,
                            'icao': airport.get('icao', ''),
                            'name': airport.get('name', ''),
                            'type': airport.get('type', ''),
                            'latitude': airport.get('geometry', {}).get('coordinates', [None, None])[1],
                            'longitude': airport.get('geometry', {}).get('coordinates', [None, None])[0],
                            'elevation': airport.get('elevation', ''),
                            'frequencies': airport.get('frequencies', []),
                            'runways': airport.get('runways', [])
                        })
                    
                    time.sleep(1)  # Be respectful to the API
                    
                except Exception as e:
                    logger.error(f"Error fetching airports for {country_code}: {e}")
                    continue
            
            return pd.DataFrame(airport_data)
            
        except Exception as e:
            logger.error(f"Error accessing OpenAIP: {e}")
            return pd.DataFrame()
    
    def merge_foundation_data(self, ourairports_data, openflights_data, openaip_data):
        """Merge data from all foundation sources"""
        logger.info("Phase 1: Merging foundation data...")
        
        if not ourairports_data or 'airports' not in ourairports_data:
            logger.error("No OurAirports data available")
            return pd.DataFrame()
        
        # Start with OurAirports as base
        merged_data = ourairports_data['airports'].copy()
        
        # Add OpenFlights data
        if 'airports' in openflights_data:
            openflights_df = openflights_data['airports']
            merged_data = merged_data.merge(
                openflights_df[['icao', 'id', 'timezone', 'dst']], 
                left_on='ident', 
                right_on='icao', 
                how='left', 
                suffixes=('', '_openflights')
            )
        
        # Add OpenAIP data
        if not openaip_data.empty:
            merged_data = merged_data.merge(
                openaip_data[['icao', 'frequencies', 'runways']], 
                left_on='ident',
                right_on='icao', 
                how='left', 
                suffixes=('', '_openaip')
            )
        
        # Clean and filter
        merged_data = merged_data.dropna(subset=['ident'])
        merged_data = merged_data[merged_data['type'].isin(['large_airport', 'medium_airport', 'small_airport'])]
        
        logger.info(f"Merged dataset contains {len(merged_data)} airports")
        return merged_data
    
    # ============ PHASE 2: DETAILED FACILITIES SCRAPING ============
    
    def find_airport_website(self, icao: str, airport_name: str) -> Optional[str]:
        """Find the official airport website"""
        # Try common patterns first (faster)
        potential_urls = [
            f"https://www.{icao.lower()}.com",
            f"https://www.{icao.lower()}airport.com",
            f"https://www.{icao.lower()}-airport.com",
            f"https://{icao.lower()}.aero",
            f"https://www.{icao.lower()}.org"
        ]
        
        for url in potential_urls:
            try:
                response = self.session.get(url, timeout=10)
                if response.status_code == 200 and 'airport' in response.text.lower():
                    return url
            except:
                continue
        
        return None
    
    def extract_service_providers(self, text_content: str, soup: BeautifulSoup, keywords: List[str]) -> List[str]:
        """Extract service providers based on keywords"""
        providers = []
        
        for keyword in keywords:
            if keyword in text_content:
                # Look for company names near the keyword
                sentences = text_content.split('.')
                for sentence in sentences:
                    if keyword in sentence:
                        # Extract potential company names
                        words = sentence.split()
                        for i, word in enumerate(words):
                            if keyword in word.lower():
                                context_words = words[max(0, i-5):i+5]
                                for context_word in context_words:
                                    if context_word.istitle() and len(context_word) > 3:
                                        providers.append(context_word)
        
        # Remove duplicates and clean
        providers = list(set([p.strip() for p in providers if p.strip()]))
        return providers[:5]
    
    def extract_contact_info(self, soup: BeautifulSoup) -> Dict[str, str]:
        """Extract contact information"""
        contact_info = {}
        
        # Email addresses
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, soup.get_text())
        if emails:
            contact_info['emails'] = emails[:3]
        
        # Phone numbers
        phone_pattern = r'\+?[\d\s\-\(\)]{10,}'
        phones = re.findall(phone_pattern, soup.get_text())
        if phones:
            contact_info['phones'] = phones[:3]
        
        return contact_info
    
    def scrape_airport_facilities(self, icao: str, airport_name: str) -> Optional[FacilityInfo]:
        """Scrape detailed facilities for a single airport"""
        website_url = self.find_airport_website(icao, airport_name)
        
        if not website_url:
            return None
        
        try:
            response = self.session.get(website_url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            text_content = soup.get_text().lower()
            
            # Extract different service types
            ground_handlers = self.extract_service_providers(text_content, soup, self.ground_handlers)
            maintenance_providers = self.extract_service_providers(text_content, soup, self.maintenance_providers)
            fuel_suppliers = self.extract_service_providers(text_content, soup, self.fuel_suppliers)
            
            # Extract other services
            cargo_keywords = ['cargo', 'freight', 'dhl', 'fedex', 'ups']
            cargo_handlers = self.extract_service_providers(text_content, soup, cargo_keywords)
            
            catering_keywords = ['catering', 'gate gourmet', 'lsg sky chefs']
            catering_services = self.extract_service_providers(text_content, soup, catering_keywords)
            
            fbo_keywords = ['fbo', 'general aviation', 'private jet']
            fbo_services = self.extract_service_providers(text_content, soup, fbo_keywords)
            
            contact_info = self.extract_contact_info(soup)
            
            return FacilityInfo(
                icao=icao,
                airport_name=airport_name,
                ground_handlers=ground_handlers,
                maintenance_providers=maintenance_providers,
                fuel_suppliers=fuel_suppliers,
                cargo_handlers=cargo_handlers,
                catering_services=catering_services,
                fbo_services=fbo_services,
                customs_hours="",
                immigration_services="",
                contact_info=contact_info,
                operating_hours="",
                source_url=website_url,
                last_updated=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.error(f"Error scraping facilities for {airport_name}: {e}")
            return None
    
    def run_detailed_facilities_scraping(self, airport_data: pd.DataFrame, max_airports: int = 100):
        """Run detailed facilities scraping for selected airports"""
        logger.info(f"Phase 2: Starting detailed facilities scraping...")
        
        # Filter and prioritize airports
        target_airports = airport_data[
            (airport_data['type'].isin(['large_airport', 'medium_airport'])) &
            (airport_data['ident'].notna()) &
            (airport_data['name'].notna())
        ].head(max_airports)
        
        logger.info(f"Selected {len(target_airports)} airports for detailed scraping")
        
        facilities_data = []
        
        for idx, airport in target_airports.iterrows():
            icao = airport['ident']
            name = airport['name']
            
            logger.info(f"Scraping facilities for {name} ({icao}) - {idx+1}/{len(target_airports)}")
            
            facility_info = self.scrape_airport_facilities(icao, name)
            if facility_info:
                facilities_data.append(facility_info)
            
            # Be respectful with delays
            time.sleep(2)
        
        logger.info(f"Completed detailed scraping for {len(facilities_data)} airports")
        return facilities_data
    
    # ============ DATA SAVING AND MANAGEMENT ============
    
    def save_data(self, data, filename_prefix: str):
        """Save data to multiple formats"""
        logger.info(f"Saving {filename_prefix} data...")
        
        filepath_base = os.path.join(self.output_dir, filename_prefix)
        
        # Save as CSV
        data.to_csv(f"{filepath_base}.csv", index=False)
        
        # Save as JSON
        data.to_json(f"{filepath_base}.json", orient='records', indent=2)
        
        # Save as Excel
        data.to_excel(f"{filepath_base}.xlsx", index=False)
        
        logger.info(f"Saved {filename_prefix} data in 3 formats")
    
    def save_facilities_data(self, facilities_data: List[FacilityInfo], filename_prefix: str):
        """Save facilities data to multiple formats"""
        logger.info(f"Saving {len(facilities_data)} facilities records...")
        
        # Convert to DataFrame
        data_dicts = []
        for facility in facilities_data:
            data_dict = {
                'icao': facility.icao,
                'airport_name': facility.airport_name,
                'ground_handlers': '; '.join(facility.ground_handlers),
                'maintenance_providers': '; '.join(facility.maintenance_providers),
                'fuel_suppliers': '; '.join(facility.fuel_suppliers),
                'cargo_handlers': '; '.join(facility.cargo_handlers),
                'catering_services': '; '.join(facility.catering_services),
                'fbo_services': '; '.join(facility.fbo_services),
                'customs_hours': facility.customs_hours,
                'immigration_services': facility.immigration_services,
                'contact_emails': '; '.join(facility.contact_info.get('emails', [])),
                'contact_phones': '; '.join(facility.contact_info.get('phones', [])),
                'operating_hours': facility.operating_hours,
                'source_url': facility.source_url,
                'last_updated': facility.last_updated
            }
            data_dicts.append(data_dict)
        
        df = pd.DataFrame(data_dicts)
        self.save_data(df, filename_prefix)
    
    def generate_summary_report(self, foundation_data: pd.DataFrame, facilities_data: List[FacilityInfo]):
        """Generate a summary report of the scraping results"""
        logger.info("Generating summary report...")
        
        report = {
            'scraping_date': datetime.now().isoformat(),
            'foundation_data': {
                'total_airports': len(foundation_data),
                'large_airports': len(foundation_data[foundation_data['type'] == 'large_airport']),
                'medium_airports': len(foundation_data[foundation_data['type'] == 'medium_airport']),
                'small_airports': len(foundation_data[foundation_data['type'] == 'small_airport']),
                'countries_covered': foundation_data['iso_country'].nunique(),
                'data_sources': ['OurAirports', 'OpenFlights', 'OpenAIP']
            },
            'facilities_data': {
                'airports_with_facilities': len(facilities_data),
                'airports_with_ground_handlers': len([f for f in facilities_data if f.ground_handlers]),
                'airports_with_maintenance': len([f for f in facilities_data if f.maintenance_providers]),
                'airports_with_fuel': len([f for f in facilities_data if f.fuel_suppliers]),
                'airports_with_cargo': len([f for f in facilities_data if f.cargo_handlers]),
                'airports_with_contact_info': len([f for f in facilities_data if f.contact_info])
            }
        }
        
        # Save report
        report_path = os.path.join(self.output_dir, 'scraping_summary.json')
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info("Summary report saved")
        return report
    
    # ============ MAIN EXECUTION METHOD ============
    
    def run_complete_scraping(self, max_detailed_airports: int = 100):
        """Run the complete scraping pipeline"""
        logger.info("=" * 60)
        logger.info("STARTING COMPLETE AIRPORT DATA SCRAPING PIPELINE")
        logger.info("=" * 60)
        
        start_time = datetime.now()
        
        try:
            # PHASE 1: Foundation Data Collection
            logger.info("PHASE 1: Foundation Data Collection")
            logger.info("-" * 40)
            
            ourairports_data = self.download_ourairports_data()
            openflights_data = self.download_openflights_data()
            openaip_data = self.scrape_openaip_data()
            
            # Merge foundation data
            foundation_data = self.merge_foundation_data(ourairports_data, openflights_data, openaip_data)
            
            if foundation_data.empty:
                logger.error("No foundation data collected. Exiting.")
                return None, None
            
            # Save foundation data
            self.save_data(foundation_data, 'foundation_airport_data')
            
            # PHASE 2: Detailed Facilities Scraping
            logger.info("\nPHASE 2: Detailed Facilities Scraping")
            logger.info("-" * 40)
            
            facilities_data = self.run_detailed_facilities_scraping(foundation_data, max_detailed_airports)
            
            # Save facilities data
            if facilities_data:
                self.save_facilities_data(facilities_data, 'detailed_airport_facilities')
            
            # Generate summary report
            report = self.generate_summary_report(foundation_data, facilities_data)
            
            # Final summary
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info("=" * 60)
            logger.info("SCRAPING COMPLETE!")
            logger.info(f"Duration: {duration}")
            logger.info(f"Foundation airports: {len(foundation_data)}")
            logger.info(f"Detailed facilities: {len(facilities_data)}")
            logger.info(f"Output directory: {self.output_dir}")
            logger.info("=" * 60)
            
            return foundation_data, facilities_data
            
        except Exception as e:
            logger.error(f"Error in complete scraping: {e}")
            return None, None

# ============ USAGE EXAMPLE ============

if __name__ == "__main__":
    # Create and run the scraper
    scraper = CombinedAirportScraper()
    
    # Run complete scraping (adjust max_detailed_airports as needed)
    foundation_data, facilities_data = scraper.run_complete_scraping(max_detailed_airports=50)
    
    if foundation_data is not None:
        print(f"\nSUCCESS!")
        print(f"Foundation data: {len(foundation_data)} airports")
        print(f"Detailed facilities: {len(facilities_data)} airports")
        print(f"Check the '{scraper.output_dir}' folder for all output files")
        
        # Display sample of foundation data
        print("\nSample foundation data columns:")
        print(foundation_data.columns.tolist())
        
        # Display sample facilities data
        if facilities_data:
            print(f"\nSample facilities data for {facilities_data[0].airport_name}:")
            print(f"Ground handlers: {facilities_data[0].ground_handlers}")
            print(f"Maintenance: {facilities_data[0].maintenance_providers}")
            print(f"Fuel suppliers: {facilities_data[0].fuel_suppliers}")
    else:
        print("Scraping failed. Check the logs for details.")