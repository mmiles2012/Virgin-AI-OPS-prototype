#!/usr/bin/env python3
"""
Aviation Data Web Scraper for Operations Center - Fixed Version
Scrapes NOTAMs, SIGMETs, TFRs, and other aviation alerts from accessible public sources
Updated URLs and improved error handling
"""

import requests
from bs4 import BeautifulSoup
import json
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import time
import re
from typing import Dict, List, Optional
import logging
from urllib.parse import urljoin, urlparse
import csv
from dataclasses import dataclass, asdict
import os

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('aviation_scraper.log')
    ]
)
logger = logging.getLogger(__name__)

# Configuration
REPLIT_MODE = os.getenv('REPLIT_DEPLOYMENT_ID') is not None
DEFAULT_TIMEOUT = 15 if REPLIT_MODE else 30
RATE_LIMIT_DELAY = 2

@dataclass
class AviationAlert:
    """Standard aviation alert data structure"""
    alert_type: str
    id: str
    location: str
    description: str
    effective_start: str
    effective_end: str
    severity: str
    source: str
    raw_data: str
    scraped_at: str

class AviationDataScraper:
    """Main scraper class for aviation data"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        self.alerts = []
        self.timeout = DEFAULT_TIMEOUT
        
        # Create output directory
        os.makedirs('output', exist_ok=True)
        
    def scrape_aviation_weather_center(self) -> List[AviationAlert]:
        """Scrape current conditions from Aviation Weather Center"""
        logger.info("Scraping Aviation Weather Center...")
        alerts = []
        
        try:
            # Main AWC page - more reliable
            url = "https://aviationweather.gov/data/api/products/text/sigmet"
            response = self.session.get(url, timeout=self.timeout)
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    if isinstance(data, list):
                        for item in data:
                            alert = AviationAlert(
                                alert_type="SIGMET",
                                id=item.get('id', f"SIGMET_{datetime.now().strftime('%Y%m%d_%H%M%S')}"),
                                location=item.get('location', 'US'),
                                description=item.get('rawText', 'SIGMET Alert'),
                                effective_start=item.get('validTimeFrom', datetime.now().isoformat()),
                                effective_end=item.get('validTimeTo', (datetime.now() + timedelta(hours=6)).isoformat()),
                                severity="HIGH",
                                source="AWC SIGMET API",
                                raw_data=json.dumps(item),
                                scraped_at=datetime.now().isoformat()
                            )
                            alerts.append(alert)
                except (json.JSONDecodeError, KeyError):
                    logger.warning("Could not parse AWC API response as JSON")
            
            # Fallback to HTML parsing
            if not alerts:
                url = "https://aviationweather.gov/"
                response = self.session.get(url, timeout=self.timeout)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Look for weather alerts
                    alert_sections = soup.find_all(['div', 'section'], class_=re.compile(r'alert|warning|notice', re.I))
                    
                    for section in alert_sections:
                        text = section.get_text(strip=True)
                        if len(text) > 50:
                            alert = AviationAlert(
                                alert_type="WEATHER_ALERT",
                                id=f"AWC_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(alerts)}",
                                location="US",
                                description=text[:500],
                                effective_start=datetime.now().isoformat(),
                                effective_end=(datetime.now() + timedelta(hours=6)).isoformat(),
                                severity="MEDIUM",
                                source="AWC Website",
                                raw_data=text,
                                scraped_at=datetime.now().isoformat()
                            )
                            alerts.append(alert)
                            
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error scraping AWC: {e}")
        except Exception as e:
            logger.error(f"Error scraping AWC: {e}")
            
        return alerts
    
    def scrape_faa_tfr_data(self) -> List[AviationAlert]:
        """Scrape TFR data from alternative sources"""
        logger.info("Scraping TFR data...")
        alerts = []
        
        try:
            # Try multiple TFR sources
            urls = [
                "https://tfr.faa.gov/save_pages/list.html",
                "https://tfr.faa.gov/save_pages/detail_list.html"
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=self.timeout)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # Look for TFR information
                        tfr_elements = soup.find_all(['div', 'tr', 'td'], string=re.compile(r'TFR|NOTAM|(\d+/\d+)', re.I))
                        
                        for element in tfr_elements:
                            parent = element.find_parent()
                            if parent:
                                text = parent.get_text(strip=True)
                                if len(text) > 30 and 'TFR' in text.upper():
                                    alert = AviationAlert(
                                        alert_type="TFR",
                                        id=f"TFR_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(alerts)}",
                                        location="US",
                                        description=text[:500],
                                        effective_start=datetime.now().isoformat(),
                                        effective_end=(datetime.now() + timedelta(days=1)).isoformat(),
                                        severity="HIGH",
                                        source="FAA TFR",
                                        raw_data=text,
                                        scraped_at=datetime.now().isoformat()
                                    )
                                    alerts.append(alert)
                        
                        if alerts:
                            break  # Stop if we found some alerts
                            
                except requests.exceptions.RequestException as e:
                    logger.warning(f"Could not access {url}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error scraping TFR data: {e}")
            
        return alerts
    
    def scrape_faa_notam_search(self) -> List[AviationAlert]:
        """Try to scrape NOTAMs from FAA NOTAM Search"""
        logger.info("Scraping NOTAM data...")
        alerts = []
        
        try:
            # FAA NOTAM Search alternative endpoints
            urls = [
                "https://notams.aim.faa.gov/notamSearch/",
                "https://pilotweb.nas.faa.gov/PilotWeb/"
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=self.timeout)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # Look for NOTAM-related content
                        notam_elements = soup.find_all(string=re.compile(r'NOTAM|!', re.I))
                        
                        for element in notam_elements:
                            if hasattr(element, 'parent'):
                                parent = element.parent
                                text = parent.get_text(strip=True)
                                if len(text) > 50 and any(keyword in text.upper() for keyword in ['NOTAM', 'CLOSED', 'RUNWAY', 'TAXIWAY']):
                                    alert = AviationAlert(
                                        alert_type="NOTAM",
                                        id=f"NOTAM_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(alerts)}",
                                        location="Various",
                                        description=text[:500],
                                        effective_start=datetime.now().isoformat(),
                                        effective_end=(datetime.now() + timedelta(days=1)).isoformat(),
                                        severity="MEDIUM",
                                        source="FAA NOTAM",
                                        raw_data=text,
                                        scraped_at=datetime.now().isoformat()
                                    )
                                    alerts.append(alert)
                        
                        if alerts:
                            break
                            
                except requests.exceptions.RequestException as e:
                    logger.warning(f"Could not access {url}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error scraping NOTAM data: {e}")
            
        return alerts
    
    def scrape_weather_gov_aviation(self) -> List[AviationAlert]:
        """Scrape aviation weather from weather.gov"""
        logger.info("Scraping weather.gov aviation data...")
        alerts = []
        
        try:
            url = "https://www.weather.gov/aviation/"
            response = self.session.get(url, timeout=self.timeout)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Look for aviation-related alerts
                alert_elements = soup.find_all(['div', 'section'], class_=re.compile(r'alert|warning|watch', re.I))
                
                for element in alert_elements:
                    text = element.get_text(strip=True)
                    if len(text) > 50 and any(keyword in text.upper() for keyword in ['AVIATION', 'TURBULENCE', 'ICING', 'CONVECTIVE']):
                        alert = AviationAlert(
                            alert_type="AVIATION_WEATHER",
                            id=f"WX_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(alerts)}",
                            location="US",
                            description=text[:500],
                            effective_start=datetime.now().isoformat(),
                            effective_end=(datetime.now() + timedelta(hours=12)).isoformat(),
                            severity="MEDIUM",
                            source="Weather.gov Aviation",
                            raw_data=text,
                            scraped_at=datetime.now().isoformat()
                        )
                        alerts.append(alert)
                        
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error scraping weather.gov: {e}")
        except Exception as e:
            logger.error(f"Error scraping weather.gov aviation: {e}")
            
        return alerts
    
    def scrape_noaa_space_weather(self) -> List[AviationAlert]:
        """Scrape space weather from NOAA"""
        logger.info("Scraping NOAA space weather...")
        alerts = []
        
        try:
            url = "https://www.swpc.noaa.gov/alerts-watches-and-warnings"
            response = self.session.get(url, timeout=self.timeout)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Look for space weather alerts
                alert_elements = soup.find_all(['div', 'article', 'section'])
                
                for element in alert_elements:
                    text = element.get_text(strip=True)
                    if len(text) > 50 and any(keyword in text.upper() for keyword in ['SOLAR', 'GEOMAGNETIC', 'RADIATION', 'STORM']):
                        alert = AviationAlert(
                            alert_type="SPACE_WEATHER",
                            id=f"SWX_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(alerts)}",
                            location="Global",
                            description=text[:500],
                            effective_start=datetime.now().isoformat(),
                            effective_end=(datetime.now() + timedelta(hours=24)).isoformat(),
                            severity="MEDIUM",
                            source="NOAA Space Weather",
                            raw_data=text,
                            scraped_at=datetime.now().isoformat()
                        )
                        alerts.append(alert)
                        
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error scraping NOAA: {e}")
        except Exception as e:
            logger.error(f"Error scraping NOAA space weather: {e}")
            
        return alerts
    
    def create_sample_alerts(self) -> List[AviationAlert]:
        """Create sample alerts for demonstration when scraping fails"""
        logger.info("Creating sample aviation alerts for demonstration...")
        
        sample_alerts = [
            AviationAlert(
                alert_type="TFR",
                id="TFR_2024_001",
                location="Washington, DC",
                description="Temporary Flight Restriction in effect for Presidential movement. No aircraft operations below 18,000 feet within 30 NM radius.",
                effective_start=datetime.now().isoformat(),
                effective_end=(datetime.now() + timedelta(hours=6)).isoformat(),
                severity="HIGH",
                source="Sample Data",
                raw_data="TFR NOTAM - Presidential TFR active",
                scraped_at=datetime.now().isoformat()
            ),
            AviationAlert(
                alert_type="SIGMET",
                id="SIGMET_CHARLIE_5",
                location="Central Plains",
                description="Severe turbulence expected below 10,000 feet due to strong low-level wind shear. Embedded thunderstorms with tops to 45,000 feet.",
                effective_start=datetime.now().isoformat(),
                effective_end=(datetime.now() + timedelta(hours=4)).isoformat(),
                severity="HIGH",
                source="Sample Data",
                raw_data="SIGMET CHARLIE 5 - Severe turbulence and thunderstorms",
                scraped_at=datetime.now().isoformat()
            ),
            AviationAlert(
                alert_type="AIRMET",
                id="AIRMET_SIERRA_3",
                location="Northeast US",
                description="IFR conditions expected due to low ceilings and visibility in mist and fog. Conditions improving after 18Z.",
                effective_start=datetime.now().isoformat(),
                effective_end=(datetime.now() + timedelta(hours=8)).isoformat(),
                severity="MEDIUM",
                source="Sample Data",
                raw_data="AIRMET SIERRA 3 - IFR conditions",
                scraped_at=datetime.now().isoformat()
            ),
            AviationAlert(
                alert_type="NOTAM",
                id="NOTAM_KLAX_001",
                location="Los Angeles, CA (KLAX)",
                description="Runway 25L closed for maintenance. Expect delays. Use Runway 25R for departures.",
                effective_start=datetime.now().isoformat(),
                effective_end=(datetime.now() + timedelta(hours=12)).isoformat(),
                severity="MEDIUM",
                source="Sample Data",
                raw_data="NOTAM - KLAX RWY 25L closed",
                scraped_at=datetime.now().isoformat()
            ),
            AviationAlert(
                alert_type="SPACE_WEATHER",
                id="SWX_G2_WATCH",
                location="Global",
                description="Geomagnetic storm watch in effect. Moderate geomagnetic storming possible. GPS and HF radio communications may be affected.",
                effective_start=datetime.now().isoformat(),
                effective_end=(datetime.now() + timedelta(hours=24)).isoformat(),
                severity="MEDIUM",
                source="Sample Data",
                raw_data="Space Weather - G2 Geomagnetic Storm Watch",
                scraped_at=datetime.now().isoformat()
            )
        ]
        
        return sample_alerts
    
    def scrape_all_data(self) -> List[AviationAlert]:
        """Scrape all aviation data sources with fallback to samples"""
        logger.info("Starting comprehensive aviation data scraping...")
        
        all_alerts = []
        
        # Define scrapers with their names
        scrapers = [
            ("Aviation Weather Center", self.scrape_aviation_weather_center),
            ("FAA TFR", self.scrape_faa_tfr_data),
            ("FAA NOTAM", self.scrape_faa_notam_search),
            ("Weather.gov Aviation", self.scrape_weather_gov_aviation),
            ("NOAA Space Weather", self.scrape_noaa_space_weather)
        ]
        
        successful_scrapers = 0
        
        for name, scraper in scrapers:
            try:
                logger.info(f"Scraping {name}...")
                alerts = scraper()
                all_alerts.extend(alerts)
                
                if alerts:
                    successful_scrapers += 1
                    logger.info(f"✓ Retrieved {len(alerts)} alerts from {name}")
                else:
                    logger.warning(f"⚠ No alerts retrieved from {name}")
                    
                # Rate limiting
                time.sleep(RATE_LIMIT_DELAY)
                
            except Exception as e:
                logger.error(f"✗ Error scraping {name}: {e}")
                continue
        
        # If no scrapers were successful, provide sample data
        if successful_scrapers == 0:
            logger.warning("No live data sources accessible. Providing sample alerts for demonstration.")
            sample_alerts = self.create_sample_alerts()
            all_alerts.extend(sample_alerts)
            
        logger.info(f"Total alerts collected: {len(all_alerts)} (from {successful_scrapers} sources)")
        return all_alerts
    
    def save_to_json(self, alerts: List[AviationAlert], filename: str = None):
        """Save alerts to JSON file"""
        if filename is None:
            filename = f"output/aviation_alerts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            data = [asdict(alert) for alert in alerts]
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            logger.info(f"✓ Saved {len(alerts)} alerts to {filename}")
            return filename
        except Exception as e:
            logger.error(f"✗ Error saving to JSON: {e}")
            return None
    
    def save_to_csv(self, alerts: List[AviationAlert], filename: str = None):
        """Save alerts to CSV file"""
        if filename is None:
            filename = f"output/aviation_alerts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            
        try:
            if not alerts:
                logger.warning("No alerts to save")
                return None
                
            fieldnames = list(asdict(alerts[0]).keys())
            with open(filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for alert in alerts:
                    writer.writerow(asdict(alert))
            logger.info(f"✓ Saved {len(alerts)} alerts to {filename}")
            return filename
        except Exception as e:
            logger.error(f"✗ Error saving to CSV: {e}")
            return None
    
    def filter_alerts(self, alerts: List[AviationAlert], 
                     alert_types: Optional[List[str]] = None,
                     severity: Optional[str] = None,
                     location_filter: Optional[str] = None) -> List[AviationAlert]:
        """Filter alerts based on criteria"""
        filtered_alerts = alerts
        
        if alert_types:
            filtered_alerts = [a for a in filtered_alerts if a.alert_type in alert_types]
            
        if severity:
            filtered_alerts = [a for a in filtered_alerts if a.severity == severity]
            
        if location_filter:
            filtered_alerts = [a for a in filtered_alerts if location_filter.upper() in a.location.upper()]
            
        return filtered_alerts

def main():
    """Main execution function"""
    print("🛩️  Aviation Data Scraper - Operations Center Edition (Fixed)")
    print("="*65)
    
    scraper = AviationDataScraper()
    
    print("Starting data collection...")
    start_time = time.time()
    
    # Scrape all data
    alerts = scraper.scrape_all_data()
    
    end_time = time.time()
    print(f"Data collection completed in {end_time - start_time:.2f} seconds")
    
    if alerts:
        print(f"\n📊 COLLECTED {len(alerts)} TOTAL ALERTS")
        
        # Save files
        json_file = scraper.save_to_json(alerts)
        csv_file = scraper.save_to_csv(alerts)
        
        if json_file:
            print(f"📄 JSON saved: {json_file}")
        if csv_file:
            print(f"📄 CSV saved: {csv_file}")
        
        # Show high severity alerts
        high_severity = scraper.filter_alerts(alerts, severity="HIGH")
        if high_severity:
            print(f"\n🚨 HIGH SEVERITY ALERTS ({len(high_severity)}):")
            print("-" * 60)
            for i, alert in enumerate(high_severity[:5], 1):
                print(f"{i}. [{alert.alert_type}] {alert.description[:100]}...")
            
            if len(high_severity) > 5:
                print(f"... and {len(high_severity) - 5} more high severity alerts")
        
        # Summary by type
        alert_types = {}
        for alert in alerts:
            alert_types[alert.alert_type] = alert_types.get(alert.alert_type, 0) + 1
        
        print(f"\n📈 SUMMARY BY TYPE:")
        print("-" * 30)
        for alert_type, count in sorted(alert_types.items()):
            print(f"  {alert_type}: {count} alerts")
            
        # Show sample alerts
        print(f"\n📅 SAMPLE ALERTS:")
        print("-" * 50)
        for i, alert in enumerate(alerts[:3], 1):
            print(f"{i}. [{alert.alert_type}] {alert.severity} - {alert.description[:80]}...")
            
    else:
        print("❌ No alerts retrieved")
        
    print(f"\n💾 Log file: aviation_scraper.log")
    print("🔄 Run again anytime to get fresh data!")

# Interactive functions for Replit
def run_interactive():
    """Interactive mode"""
    print("🛩️  Aviation Data Scraper - Interactive Mode")
    print("="*50)
    
    while True:
        print("\nOptions:")
        print("1. Scrape all aviation data")
        print("2. Test individual scrapers")
        print("3. View saved alerts")
        print("4. Generate sample data")
        print("5. Exit")
        
        choice = input("\nEnter your choice (1-5): ").strip()
        
        if choice == '1':
            main()
        elif choice == '2':
            test_scrapers()
        elif choice == '3':
            view_saved_alerts()
        elif choice == '4':
            generate_sample_data()
        elif choice == '5':
            print("👋 Goodbye!")
            break
        else:
            print("❌ Invalid choice. Please try again.")

def test_scrapers():
    """Test individual scrapers"""
    scraper = AviationDataScraper()
    
    print("\nTesting individual scrapers:")
    print("-" * 40)
    
    scrapers = [
        ("Aviation Weather Center", scraper.scrape_aviation_weather_center),
        ("FAA TFR", scraper.scrape_faa_tfr_data),
        ("Weather.gov Aviation", scraper.scrape_weather_gov_aviation),
        ("NOAA Space Weather", scraper.scrape_noaa_space_weather)
    ]
    
    for name, scraper_func in scrapers:
        print(f"\n🔍 Testing {name}...")
        try:
            alerts = scraper_func()
            if alerts:
                print(f"✓ Success: {len(alerts)} alerts found")
                print(f"  Sample: {alerts[0].description[:60]}...")
            else:
                print("⚠ No alerts found (may be normal)")
        except Exception as e:
            print(f"✗ Error: {e}")
        
        time.sleep(1)

def view_saved_alerts():
    """View saved alerts"""
    import glob
    
    files = glob.glob("output/aviation_alerts_*.json")
    if not files:
        print("No saved files found")
        return
    
    latest_file = max(files, key=os.path.getctime)
    print(f"\nLatest file: {latest_file}")
    
    try:
        with open(latest_file, 'r') as f:
            alerts = json.load(f)
        
        print(f"Found {len(alerts)} alerts")
        for i, alert in enumerate(alerts[:3], 1):
            print(f"{i}. [{alert['alert_type']}] {alert['description'][:80]}...")
            
    except Exception as e:
        print(f"Error reading file: {e}")

def generate_sample_data():
    """Generate sample data for testing"""
    scraper = AviationDataScraper()
    alerts = scraper.create_sample_alerts()
    
    json_file = scraper.save_to_json(alerts)
    csv_file = scraper.save_to_csv(alerts)
    
    print(f"\n✓ Generated {len(alerts)} sample alerts")
    print(f"📄 JSON: {json_file}")
    print(f"📄 CSV: {csv_file}")

if __name__ == "__main__":
    if REPLIT_MODE:
        run_interactive()
    else:
        main()