#!/usr/bin/env python3
"""
Aviation Data Web Scraper for Operations Center - Replit Version
Scrapes NOTAMs, SIGMETs, TFRs, and other aviation alerts from public sources
Optimized for Replit environment
"""

import requests
from bs4 import BeautifulSoup
import json
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import time
import re
from typing import Dict, List, Optional
import logging
from urllib.parse import urljoin, urlparse
import csv
from dataclasses import dataclass, asdict
import os

# Configure logging for Replit
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # Console output
        logging.FileHandler('aviation_scraper.log')  # File output
    ]
)
logger = logging.getLogger(__name__)

# Replit-specific configuration
REPLIT_MODE = os.getenv('REPLIT_DEPLOYMENT_ID') is not None
if REPLIT_MODE:
    logger.info("Running in Replit environment")
    # Set shorter timeouts for Replit
    DEFAULT_TIMEOUT = 15
    RATE_LIMIT_DELAY = 3
else:
    DEFAULT_TIMEOUT = 30
    RATE_LIMIT_DELAY = 2

@dataclass
class AviationAlert:
    """Standard aviation alert data structure"""
    alert_type: str
    id: str
    location: str
    description: str
    effective_start: str
    effective_end: str
    severity: str
    source: str
    raw_data: str
    scraped_at: str

class AviationDataScraper:
    """Main scraper class for aviation data"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.alerts = []
        self.timeout = DEFAULT_TIMEOUT
        
        # Create output directory if it doesn't exist
        os.makedirs('output', exist_ok=True)
        
    def scrape_tfr_data(self) -> List[AviationAlert]:
        """Scrape TFR (Temporary Flight Restriction) data"""
        logger.info("Scraping TFR data...")
        alerts = []
        
        try:
            # TFR data from FAA
            url = "https://tfr.faa.gov/save_pages/detail_9_8155.html"
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Parse TFR details
            tfr_blocks = soup.find_all('div', class_='tfr-detail')
            
            for block in tfr_blocks:
                try:
                    title = block.find('h3')
                    if title:
                        tfr_id = title.get_text(strip=True)
                        
                        # Extract details
                        details = block.find('div', class_='tfr-text')
                        if details:
                            description = details.get_text(strip=True)
                            
                            alert = AviationAlert(
                                alert_type="TFR",
                                id=tfr_id,
                                location="Various",
                                description=description,
                                effective_start=datetime.now().isoformat(),
                                effective_end=(datetime.now() + timedelta(days=1)).isoformat(),
                                severity="HIGH",
                                source="FAA TFR",
                                raw_data=str(block),
                                scraped_at=datetime.now().isoformat()
                            )
                            alerts.append(alert)
                            
                except Exception as e:
                    logger.warning(f"Error parsing TFR block: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error scraping TFR data: {e}")
            
        return alerts
    
    def scrape_sigmet_data(self) -> List[AviationAlert]:
        """Scrape SIGMET data from Aviation Weather Center"""
        logger.info("Scraping SIGMET data...")
        alerts = []
        
        try:
            # AWC SIGMET data
            url = "https://aviationweather.gov/sigmet/data?hazard=conv&loc=us"
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            # Try to parse as JSON first
            try:
                data = response.json()
                if isinstance(data, list):
                    for item in data:
                        alert = AviationAlert(
                            alert_type="SIGMET",
                            id=item.get('id', 'N/A'),
                            location=item.get('location', 'N/A'),
                            description=item.get('hazard', 'N/A'),
                            effective_start=item.get('validTime', datetime.now().isoformat()),
                            effective_end=item.get('validTimeEnd', (datetime.now() + timedelta(hours=6)).isoformat()),
                            severity="HIGH",
                            source="AWC SIGMET",
                            raw_data=json.dumps(item),
                            scraped_at=datetime.now().isoformat()
                        )
                        alerts.append(alert)
            except json.JSONDecodeError:
                # Parse as HTML if JSON fails
                soup = BeautifulSoup(response.content, 'html.parser')
                sigmet_elements = soup.find_all('div', class_='sigmet-item')
                
                for element in sigmet_elements:
                    try:
                        text = element.get_text(strip=True)
                        alert = AviationAlert(
                            alert_type="SIGMET",
                            id=f"SIGMET_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                            location="US",
                            description=text,
                            effective_start=datetime.now().isoformat(),
                            effective_end=(datetime.now() + timedelta(hours=6)).isoformat(),
                            severity="HIGH",
                            source="AWC SIGMET",
                            raw_data=text,
                            scraped_at=datetime.now().isoformat()
                        )
                        alerts.append(alert)
                    except Exception as e:
                        logger.warning(f"Error parsing SIGMET element: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"Error scraping SIGMET data: {e}")
            
        return alerts
    
    def scrape_airmet_data(self) -> List[AviationAlert]:
        """Scrape AIRMET data"""
        logger.info("Scraping AIRMET data...")
        alerts = []
        
        try:
            url = "https://aviationweather.gov/gairmet/data?date=&type=snapshot&region=us"
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            # Parse response
            soup = BeautifulSoup(response.content, 'html.parser')
            airmet_items = soup.find_all('pre') or soup.find_all('div', class_='airmet-text')
            
            for item in airmet_items:
                text = item.get_text(strip=True)
                if text and any(keyword in text.upper() for keyword in ['AIRMET', 'SIERRA', 'TANGO', 'ZULU']):
                    alert = AviationAlert(
                        alert_type="AIRMET",
                        id=f"AIRMET_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        location="US",
                        description=text,
                        effective_start=datetime.now().isoformat(),
                        effective_end=(datetime.now() + timedelta(hours=6)).isoformat(),
                        severity="MEDIUM",
                        source="AWC AIRMET",
                        raw_data=text,
                        scraped_at=datetime.now().isoformat()
                    )
                    alerts.append(alert)
                    
        except Exception as e:
            logger.error(f"Error scraping AIRMET data: {e}")
            
        return alerts
    
    def scrape_notam_data(self) -> List[AviationAlert]:
        """Scrape NOTAM data from available sources"""
        logger.info("Scraping NOTAM data...")
        alerts = []
        
        try:
            # Try alternative NOTAM sources since main FAA site is JavaScript-heavy
            # Aviation Weather Center sometimes includes NOTAM information
            url = "https://aviationweather.gov/fcst/hazard"
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for NOTAM-related content
            notam_elements = soup.find_all('div', string=re.compile(r'NOTAM|NOTICE', re.IGNORECASE))
            
            for element in notam_elements:
                parent = element.find_parent()
                if parent:
                    text = parent.get_text(strip=True)
                    if len(text) > 20:  # Filter out very short entries
                        alert = AviationAlert(
                            alert_type="NOTAM",
                            id=f"NOTAM_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                            location="Various",
                            description=text,
                            effective_start=datetime.now().isoformat(),
                            effective_end=(datetime.now() + timedelta(days=1)).isoformat(),
                            severity="MEDIUM",
                            source="AWC NOTAM",
                            raw_data=text,
                            scraped_at=datetime.now().isoformat()
                        )
                        alerts.append(alert)
                        
        except Exception as e:
            logger.error(f"Error scraping NOTAM data: {e}")
            
        return alerts
    
    def scrape_space_weather(self) -> List[AviationAlert]:
        """Scrape space weather alerts that affect aviation"""
        logger.info("Scraping space weather data...")
        alerts = []
        
        try:
            url = "https://aviationweather.gov/swx/"
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for space weather alerts
            swx_elements = soup.find_all('div', class_='swx-alert') or soup.find_all('pre')
            
            for element in swx_elements:
                text = element.get_text(strip=True)
                if text and any(keyword in text.upper() for keyword in ['SOLAR', 'GEOMAG', 'RADIATION', 'OUTAGE']):
                    alert = AviationAlert(
                        alert_type="SPACE_WEATHER",
                        id=f"SWX_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        location="Global",
                        description=text,
                        effective_start=datetime.now().isoformat(),
                        effective_end=(datetime.now() + timedelta(hours=24)).isoformat(),
                        severity="MEDIUM",
                        source="AWC Space Weather",
                        raw_data=text,
                        scraped_at=datetime.now().isoformat()
                    )
                    alerts.append(alert)
                    
        except Exception as e:
            logger.error(f"Error scraping space weather data: {e}")
            
        return alerts
    
    def scrape_all_data(self) -> List[AviationAlert]:
        """Scrape all aviation data sources"""
        logger.info("Starting comprehensive aviation data scraping...")
        
        all_alerts = []
        
        # Scrape different data sources
        scrapers = [
            self.scrape_tfr_data,
            self.scrape_sigmet_data,
            self.scrape_airmet_data,
            self.scrape_notam_data,
            self.scrape_space_weather
        ]
        
        for scraper in scrapers:
            try:
                alerts = scraper()
                all_alerts.extend(alerts)
                logger.info(f"Retrieved {len(alerts)} alerts from {scraper.__name__}")
                # Rate limiting - adjusted for Replit
                time.sleep(RATE_LIMIT_DELAY)
            except Exception as e:
                logger.error(f"Error in {scraper.__name__}: {e}")
                continue
                
        logger.info(f"Total alerts retrieved: {len(all_alerts)}")
        return all_alerts
    
    def save_to_json(self, alerts: List[AviationAlert], filename: str = None):
        """Save alerts to JSON file"""
        if filename is None:
            filename = f"output/aviation_alerts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            data = [asdict(alert) for alert in alerts]
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            logger.info(f"Saved {len(alerts)} alerts to {filename}")
            return filename
        except Exception as e:
            logger.error(f"Error saving to JSON: {e}")
            return None
    
    def save_to_csv(self, alerts: List[AviationAlert], filename: str = None):
        """Save alerts to CSV file"""
        if filename is None:
            filename = f"output/aviation_alerts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            
        try:
            if not alerts:
                logger.warning("No alerts to save")
                return None
                
            fieldnames = list(asdict(alerts[0]).keys())
            with open(filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for alert in alerts:
                    writer.writerow(asdict(alert))
            logger.info(f"Saved {len(alerts)} alerts to {filename}")
            return filename
        except Exception as e:
            logger.error(f"Error saving to CSV: {e}")
            return None
    
    def filter_alerts(self, alerts: List[AviationAlert], 
                     alert_types: Optional[List[str]] = None,
                     severity: Optional[str] = None,
                     location_filter: Optional[str] = None) -> List[AviationAlert]:
        """Filter alerts based on criteria"""
        filtered_alerts = alerts
        
        if alert_types:
            filtered_alerts = [a for a in filtered_alerts if a.alert_type in alert_types]
            
        if severity:
            filtered_alerts = [a for a in filtered_alerts if a.severity == severity]
            
        if location_filter:
            filtered_alerts = [a for a in filtered_alerts if location_filter.upper() in a.location.upper()]
            
        return filtered_alerts

def main():
    """Main execution function - Replit optimized"""
    print("üõ©Ô∏è  Aviation Data Scraper - Operations Center Edition")
    print("="*60)
    
    scraper = AviationDataScraper()
    
    print("Starting data collection...")
    start_time = time.time()
    
    # Scrape all data
    alerts = scraper.scrape_all_data()
    
    end_time = time.time()
    print(f"Data collection completed in {end_time - start_time:.2f} seconds")
    
    if alerts:
        print(f"\nüìä COLLECTED {len(alerts)} TOTAL ALERTS")
        
        # Save files
        json_file = scraper.save_to_json(alerts)
        csv_file = scraper.save_to_csv(alerts)
        
        if json_file:
            print(f"üìÑ JSON saved: {json_file}")
        if csv_file:
            print(f"üìÑ CSV saved: {csv_file}")
        
        # Filter for high severity alerts
        high_severity = scraper.filter_alerts(alerts, severity="HIGH")
        if high_severity:
            print(f"\nüö® HIGH SEVERITY ALERTS ({len(high_severity)}):")
            print("-" * 50)
            for i, alert in enumerate(high_severity[:5], 1):  # Show first 5
                print(f"{i}. [{alert.alert_type}] {alert.description[:80]}...")
            
            if len(high_severity) > 5:
                print(f"... and {len(high_severity) - 5} more high severity alerts")
        
        # Summary by type
        alert_types = {}
        for alert in alerts:
            alert_types[alert.alert_type] = alert_types.get(alert.alert_type, 0) + 1
        
        print(f"\nüìà SUMMARY BY TYPE:")
        print("-" * 30)
        for alert_type, count in sorted(alert_types.items()):
            print(f"  {alert_type}: {count} alerts")
            
        # Show recent alerts
        print(f"\nüìÖ RECENT ALERTS (Last 3):")
        print("-" * 40)
        for alert in alerts[-3:]:
            print(f"  [{alert.alert_type}] {alert.description[:60]}...")
            
    else:
        print("‚ùå No alerts retrieved")
        print("This might be due to:")
        print("  - Network connectivity issues")
        print("  - Website changes")
        print("  - Rate limiting")
        print("  - Check the log file for details")
        
    print(f"\nüíæ Log file: aviation_scraper.log")
    print("üîÑ Run again anytime to get fresh data!")

# Replit-specific interactive functions
def run_interactive():
    """Interactive mode for Replit"""
    print("üõ©Ô∏è  Aviation Data Scraper - Interactive Mode")
    print("="*50)
    
    while True:
        print("\nOptions:")
        print("1. Scrape all aviation data")
        print("2. Scrape specific data type")
        print("3. Start monitoring mode")
        print("4. View latest alerts")
        print("5. Exit")
        
        choice = input("\nEnter your choice (1-5): ").strip()
        
        if choice == '1':
            main()
        elif choice == '2':
            run_specific_scraper()
        elif choice == '3':
            run_monitor()
        elif choice == '4':
            view_latest_alerts()
        elif choice == '5':
            print("üëã Goodbye!")
            break
        else:
            print("‚ùå Invalid choice. Please try again.")

def run_specific_scraper():
    """Run specific scraper"""
    scraper = AviationDataScraper()
    
    print("\nSelect data type to scrape:")
    print("1. TFR (Temporary Flight Restrictions)")
    print("2. SIGMET (Significant Meteorological Information)")
    print("3. AIRMET (Airmen's Meteorological Information)")
    print("4. NOTAM (Notice to Airmen)")
    print("5. Space Weather")
    
    choice = input("Enter choice (1-5): ").strip()
    
    scraper_map = {
        '1': scraper.scrape_tfr_data,
        '2': scraper.scrape_sigmet_data,
        '3': scraper.scrape_airmet_data,
        '4': scraper.scrape_notam_data,
        '5': scraper.scrape_space_weather
    }
    
    if choice in scraper_map:
        print(f"Scraping {choice}...")
        alerts = scraper_map[choice]()
        if alerts:
            print(f"Found {len(alerts)} alerts")
            scraper.save_to_json(alerts)
            scraper.save_to_csv(alerts)
        else:
            print("No alerts found")
    else:
        print("Invalid choice")

def run_monitor():
    """Run monitoring mode with user control"""
    print("\nüîç Starting monitoring mode...")
    print("Press Ctrl+C to stop monitoring")
    
    interval = input("Check interval in seconds (default 300): ").strip()
    try:
        interval = int(interval) if interval else 300
    except ValueError:
        interval = 300
    
    monitor = AlertMonitor(check_interval=interval)
    try:
        monitor.start_monitoring()
    except KeyboardInterrupt:
        print("\n‚úÖ Monitoring stopped by user")

def view_latest_alerts():
    """View latest alerts from saved files"""
    import glob
    
    json_files = glob.glob("output/aviation_alerts_*.json")
    if not json_files:
        print("No saved alert files found")
        return
    
    # Get most recent file
    latest_file = max(json_files, key=os.path.getctime)
    
    try:
        with open(latest_file, 'r') as f:
            alerts = json.load(f)
        
        print(f"\nüìÑ Latest alerts from: {latest_file}")
        print(f"Total alerts: {len(alerts)}")
        
        # Show first few alerts
        for i, alert in enumerate(alerts[:5], 1):
            print(f"{i}. [{alert['alert_type']}] {alert['description'][:80]}...")
            
    except Exception as e:
        print(f"Error reading file: {e}")

if __name__ == "__main__":
    if REPLIT_MODE:
        # In Replit, offer interactive mode
        run_interactive()
    else:
        # Standard execution
        main()


# Additional utility functions for operations center integration

class AlertMonitor:
    """Continuous monitoring class for operations center"""
    
    def __init__(self, check_interval: int = 300):  # 5 minutes
        self.check_interval = check_interval
        self.scraper = AviationDataScraper()
        self.previous_alerts = set()
        
    def start_monitoring(self):
        """Start continuous monitoring"""
        logger.info(f"Starting aviation alert monitoring (check interval: {self.check_interval}s)")
        
        while True:
            try:
                current_alerts = self.scraper.scrape_all_data()
                
                # Check for new alerts
                current_alert_ids = {alert.id for alert in current_alerts}
                new_alerts = current_alert_ids - self.previous_alerts
                
                if new_alerts:
                    logger.info(f"Found {len(new_alerts)} new alerts")
                    # Here you would integrate with your operations center systems
                    # e.g., send to message queue, database, or alert system
                    self.handle_new_alerts([a for a in current_alerts if a.id in new_alerts])
                
                self.previous_alerts = current_alert_ids
                
                # Wait before next check
                time.sleep(self.check_interval)
                
            except KeyboardInterrupt:
                logger.info("Monitoring stopped by user")
                break
            except Exception as e:
                logger.error(f"Error in monitoring loop: {e}")
                time.sleep(60)  # Wait a minute before retrying
                
    def handle_new_alerts(self, alerts: List[AviationAlert]):
        """Handle new alerts - customize for your operations center"""
        for alert in alerts:
            # Example: Log to operations center
            logger.info(f"NEW ALERT: [{alert.alert_type}] {alert.severity} - {alert.description[:100]}...")
            
            # Example: Send to message queue, database, etc.
            # self.send_to_ops_center(alert)
            
            # Example: Send email/SMS for high severity
            if alert.severity == "HIGH":
                logger.warning(f"HIGH SEVERITY ALERT: {alert.alert_type} - {alert.description}")
                # self.send_high_priority_notification(alert)

# Example usage for operations center integration:
# monitor = AlertMonitor(check_interval=300)  # Check every 5 minutes
# monitor.start_monitoring()